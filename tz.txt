Тестовое задание для Data Engineer

Описание:
Для каждой задачи заложен базовый балл (взят в “[]” перед описанием задачи).
Минимальный проходной балл = 35.
Весь используемый стек прилагается в конце документа в виде двух приложений:
Базовая инфраструктура, которая включает в себя PostgreSQL и Kafka
Сервис Airflow

Используемый стек технологий:
GitLab, Python, PostgreSQL, Kafka, Airflow

Входные данные:
API Wildberries с запросом фразы query=куртка (можно изменить на свой запрос, который имеет не менее 10 страниц по параметру page):
https://search.wb.ru/exactmatch/ru/common/v4/search?&query=куртка&curr=rub&dest=-1257786&regions=80,64,38,4,115,83,33,68,70,69,30,86,75,40,1,66,48,110,31,22,71,114,111&resultset=catalog&sort=popular&spp=0&suppressSpellcheck=false&limit=300&page=1

Задания:
1 [10] Спроектировать структуру таблиц в базе данных под данные из ответа вышеуказанного API.
2 [20] Написать Python скрипт, который пройдется по страницам из API (параметр page) с 1 по 10 и запишет данные, которые возвращаются в ответе (все данные, включая metadata, state, version, params, data).
3 [5] Написать SQL запрос, который выведет ТОП-5 товаров с минимальным значением в поле position.
4 [5] Написать SQL запрос, который подсчитает количество товаров в каждом бренде.
5 [10] Написать SQL запрос, который подсчитает среднюю стоимость по всем существующим товарам в базе данных. Ориентироваться на поле salePriceU.
6[20] Написать скрипт (producer) для добавления сообщений в топик Kafka . Сообщение должно содержать id продукта из предыдущего пункта. Скрипт должен пройтись по всем продуктам, которые имеются в базе данных.
7 [20] Написать скрипт (consumer) и обрабатывать сообщения из топика. Обработка сообщения должна состоять из двух шагов:
	1 шаг: Записать текущую дату в колонку processed_at в таблице с продуктами по пришедшему из сообщения id продукта;
	2 шаг: Подтвердить прочтение сообщения методом Ack (в случае, если запись с заданным id не нашлась, то выполнить Nack сообщения и записать в лог с необработанными сообщениями).
8[30] Написать 2 DAG’a для Airflow, которые будут запускать скрипты из пунктов 6 и 7 по расписанию * 1 * * *
